{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ac609248240a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mslim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresnet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import resnet as rn\n",
    "import data_handler as dh\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "\n",
    "\n",
    "class Model:\n",
    "  HEIGHT = 12\n",
    "  WIDTH = 12\n",
    "  DEPTH = 45\n",
    "  LABEL_LENGTH = 38\n",
    "  BATCH_SIZE_TRAIN = 32\n",
    "  BATCH_SIZE_VAL = 32\n",
    "  BATCH_SIZE_TEST = 32\n",
    "  NUM_RESIDUAL_BLOCKS = 5\n",
    "  TRAIN_EMA_DECAY = 0.95\n",
    "  TRAIN_STEPS = 80000\n",
    "  EPOCH_SIZE = 100 \n",
    "  \n",
    "  REPORT_FREQ = 100\n",
    "  FULL_VALIDATION = False\n",
    "  INIT_LR = 0.1\n",
    "\n",
    "  DECAY_STEP_0 = 40000\n",
    "  DECAY_STEP_1 = 60000\n",
    "  \n",
    "  NUM_CLASS = 38\n",
    "\n",
    "  use_ckpt = False\n",
    "  ckpt_path = 'cache/logs/model.ckpt'\n",
    "  train_path = 'cache/train/'\n",
    "\n",
    "  def __init__(self):\n",
    "    #The data points must be given one by one here\n",
    "    #But the whole trajectory must be given to the LSTM\n",
    "    self.traj_placeholder = tf.placeholder(dtype=tf.float32, shape=[self.BATCH_SIZE_TRAIN, self.HEIGHT, self.WIDTH, self.DEPTH])\n",
    "    self.goal_placeholder = tf.placeholder(dtype=tf.int32, shape=[self.BATCH_SIZE_TRAIN])\n",
    "    self.vali_traj_placeholder = tf.placeholder(dtype=tf.float32, shape=[self.BATCH_SIZE_VAL, self.HEIGHT, self.WIDTH, self.DEPTH])\n",
    "    self.vali_goal_placeholder = tf.placeholder(dtype=tf.int32, shape=[self.BATCH_SIZE_VAL])\n",
    "    self.lr_placeholder = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "            \n",
    "  def _create_graphs(self):\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    validation_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    logits = rn.build_charnet(self.traj_placeholder, self.NUM_RESIDUAL_BLOCKS, reuse=False, train=True)\n",
    "    vali_logits = rn.build_charnet(self.vali_traj_placeholder, self.NUM_RESIDUAL_BLOCKS, reuse=True, train=True)\n",
    "    \n",
    "    regu_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    \n",
    "    #Training loss and error\n",
    "    loss = self.loss(logits, self.goal_placeholder)\n",
    "    self.full_loss = tf.add_n([loss] + regu_losses)\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "    self.train_top1_error = self.top_k_error(predictions, self.goal_placeholder, 1)\n",
    "\n",
    "    # Validation loss and error\n",
    "    self.vali_loss = self.loss(vali_logits, self.vali_goal_placeholder)\n",
    "    vali_predictions = tf.nn.softmax(vali_logits)\n",
    "    self.vali_top1_error = self.top_k_error(vali_predictions, self.vali_goal_placeholder, 1)\n",
    "\n",
    "    # Define operations\n",
    "    self.train_op, self.train_ema_op = self.train_operation(global_step, self.full_loss, self.train_top1_error)\n",
    "    self.val_op = self.validation_op(validation_step, self.vali_top1_error, self.vali_loss)\n",
    "    \n",
    "    return\n",
    "        \n",
    "  def train(self):\n",
    "    #Load data from tfrecord\n",
    "    dir = os.getcwd() + '/S001a/'\n",
    "    data_handler = dh.DataHandler(dir)\n",
    "\n",
    "    all_data, all_labels = data_handler.parse_all_trajectories(dir)\n",
    "    vali_data, vali_labels = data_handler.parse_all_trajectories(dir)\n",
    "\n",
    "    #Build graphs\n",
    "    self._create_graphs()\n",
    "\n",
    "    # Initialize a saver to save checkpoints. Merge all summaries, so we can run all\n",
    "    # summarizing operations by running summary_op. Initialize a new session\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # If you want to load from a checkpoint\n",
    "    if self.use_ckpt:\n",
    "      saver.restore(sess, self.ckpt_path)\n",
    "      print('Restored from checkpoint...')\n",
    "    else:\n",
    "      sess.run(init)\n",
    "      \n",
    "    # This summary writer object helps write summaries on tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(self.train_path, sess.graph)\n",
    "\n",
    "    # These lists are used to save a csv file at last\n",
    "    step_list = []\n",
    "    train_error_list = []\n",
    "    val_error_list = []\n",
    "        \n",
    "    print('Start training...')\n",
    "    print('----------------------------')\n",
    "\n",
    "    for step in range(self.TRAIN_STEPS):\n",
    "      #Generate batches for training and validation\n",
    "      train_batch_data, train_batch_labels = self.generate_augment_train_batch(all_data, all_labels, self.BATCH_SIZE_TRAIN)\n",
    "      validation_batch_data, validation_batch_labels = self.generate_vali_batch(vali_data, vali_labels, self.BATCH_SIZE_VAL)\n",
    "\n",
    "      #Validate first?\n",
    "      if step % self.REPORT_FREQ == 0:\n",
    "        if self.FULL_VALIDATION:\n",
    "          validation_loss_value, validation_error_value = self.full_validation(loss=self.vali_loss, top1_error=self.vali_top1_error, vali_data=vali_data, vali_labels=vali_labels, session=sess, batch_data=train_batch_data, batch_label=train_batch_labels)\n",
    "\n",
    "          vali_summ = tf.Summary()\n",
    "          vali_summ.value.add(tag='full_validation_error', simple_value=validation_error_value.astype(np.float))\n",
    "          summary_writer.add_summary(vali_summ, step)\n",
    "          summary_writer.flush()\n",
    "        \n",
    "        else:\n",
    "          _, validation_error_value, validation_loss_value = sess.run([self.val_op, self.vali_top1_error, self.vali_loss], {self.traj_placeholder: train_batch_data, self.goal_placeholder: train_batch_labels, self.vali_traj_placeholder: validation_batch_data, self.vali_goal_placeholder: validation_batch_labels, self.lr_placeholder: self.INIT_LR})\n",
    "        \n",
    "        val_error_list.append(validation_error_value)\n",
    "      \n",
    "      start_time = time.time()\n",
    "\n",
    "      #Actual training\n",
    "      _, _, train_loss_value, train_error_value = sess.run([self.train_op, self.train_ema_op,\n",
    "                                                            self.full_loss, self.train_top1_error],\n",
    "                                                            {self.traj_placeholder: train_batch_data,\n",
    "                                                            self.goal_placeholder: train_batch_labels,\n",
    "                                                            self.vali_traj_placeholder: validation_batch_data,\n",
    "                                                            self.vali_goal_placeholder: validation_batch_labels,\n",
    "                                                            self.lr_placeholder: self.INIT_LR})\n",
    "      duration = time.time() - start_time\n",
    "\n",
    "      if step % self.REPORT_FREQ == 0:\n",
    "        summary_str = sess.run(summary_op, {self.traj_placeholder: train_batch_data,\n",
    "                                            self.goal_placeholder: train_batch_labels,\n",
    "                                            self.vali_traj_placeholder: validation_batch_data,\n",
    "                                            self.vali_goal_placeholder: validation_batch_labels,\n",
    "                                            self.lr_placeholder: self.INIT_LR})\n",
    "        summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "        num_examples_per_step = self.BATCH_SIZE_TRAIN\n",
    "        examples_per_sec = num_examples_per_step / duration\n",
    "        sec_per_batch = float(duration)\n",
    "\n",
    "        format_str = ('%s: step %d, loss = %.4f (%.1f examples/sec; %.3f ' 'sec/batch)')\n",
    "        print(format_str % (datetime.datetime.now(), step, train_loss_value, examples_per_sec, sec_per_batch))\n",
    "        print('Train top1 error = ', train_error_value)\n",
    "        print('Validation top1 error = %.4f' % validation_error_value)\n",
    "        print('Validation loss = ', validation_loss_value)\n",
    "        print('----------------------------')\n",
    "\n",
    "        step_list.append(step)\n",
    "        train_error_list.append(train_error_value)\n",
    "            \n",
    "      if step == self.DECAY_STEP_0 or step == self.DECAY_STEP_1:\n",
    "        self.INIT_LR = 0.1 * self.INIT_LR\n",
    "        print('Learning rate decayed to ', self.INIT_LR)\n",
    "\n",
    "      # Save checkpoints every 10000 steps\n",
    "      if step % 10000 == 0 or (step + 1) == self.TRAIN_STEPS:\n",
    "          checkpoint_path = os.path.join(self.train_path, 'model.ckpt')\n",
    "          saver.save(sess, checkpoint_path, global_step=step)\n",
    "\n",
    "          df = pd.DataFrame(data={'step':step_list, 'train_error':train_error_list,\n",
    "                          'validation_error': val_error_list})\n",
    "          df.to_csv(self.train_path + '_error.csv')\n",
    "\n",
    "  def test(self, test_image_array):\n",
    "    '''\n",
    "    This function is used to evaluate the test data. Please finish pre-precessing in advance\n",
    "\n",
    "    :param test_image_array: 4D numpy array with shape [num_test_images, img_height, img_width,\n",
    "    img_depth]\n",
    "    :return: the softmax probability with shape [num_test_images, num_labels]\n",
    "    '''\n",
    "    num_test_images = len(test_image_array)\n",
    "    num_batches = num_test_images // self.BATCH_SIZE_TEST\n",
    "    remain_images = num_test_images % self.BATCH_SIZE_TEST\n",
    "    print('%i test batches in total...' %num_batches)\n",
    "\n",
    "    # Create the test image and labels placeholders\n",
    "    self.test_traj_placeholder = tf.placeholder(dtype=tf.float32, shape=[self.BATCH_SIZE_TEST, self.HEIGHT, self.WIDTH, self.DEPTH])\n",
    "\n",
    "    # Build the test graph\n",
    "    logits = rn.build_charnet(self.test_traj_placeholder, self.NUM_RESIDUAL_BLOCKS, reuse=False, train=False)\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "\n",
    "    # Initialize a new session and restore a checkpoint\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "    sess = tf.Session()\n",
    "\n",
    "    saver.restore(sess, self.ckpt_path)\n",
    "    print('Model restored from ', self.ckpt_path)\n",
    "\n",
    "    prediction_array = np.array([]).reshape(-1, self.NUM_CLASS)\n",
    "\n",
    "    # Test by batches\n",
    "    for step in range(num_batches):\n",
    "      if step % 10 == 0:\n",
    "          print('%i batches finished!' %step)\n",
    "      offset = step * self.BATCH_SIZE_TEST\n",
    "      test_image_batch = test_image_array[offset:offset+self.BATCH_SIZE_TEST, ...]\n",
    "\n",
    "      batch_prediction_array = sess.run(predictions, feed_dict={self.test_traj_placeholder: test_image_batch})\n",
    "      prediction_array = np.concatenate((prediction_array, batch_prediction_array))\n",
    "\n",
    "    # If test_batch_size is not a divisor of num_test_images\n",
    "    if remain_images != 0:\n",
    "      self.test_image_placeholder = tf.placeholder(dtype=tf.float32, shape=[remain_images, self.HEIGHT, self.WIDTH, self.DEPTH])\n",
    "      # Build the test graph\n",
    "      logits = rn.build_charnet(self.test_traj_placeholder, self.NUM_RESIDUAL_BLOCKS, reuse=True)\n",
    "      predictions = tf.nn.softmax(logits)\n",
    "\n",
    "      test_image_batch = test_image_array[-remain_images:, ...]\n",
    "\n",
    "      batch_prediction_array = sess.run(predictions, feed_dict={self.test_image_placeholder: test_image_batch})\n",
    "\n",
    "      prediction_array = np.concatenate((prediction_array, batch_prediction_array))\n",
    "\n",
    "    return prediction_array\n",
    "  \n",
    "  def loss(self, logits, labels):\n",
    "    '''\n",
    "    Calculate the cross entropy loss given logits and true labels\n",
    "    :param logits: 2D tensor with shape [batch_size, num_labels]\n",
    "    :param labels: 1D tensor with shape [batch_size]\n",
    "    :return: loss tensor with shape [1]\n",
    "    '''\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                                    labels=labels, name='cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "    return cross_entropy_mean\n",
    "\n",
    "  def top_k_error(self, predictions, labels, k):\n",
    "    '''\n",
    "    Calculate the top-k error\n",
    "    :param predictions: 2D tensor with shape [batch_size, num_labels]\n",
    "    :param labels: 1D tensor with shape [batch_size, 1]\n",
    "    :param k: int\n",
    "    :return: tensor with shape [1]\n",
    "    '''\n",
    "    batch_size = predictions.get_shape().as_list()[0]\n",
    "    in_top1 = tf.to_float(tf.nn.in_top_k(predictions, labels, k=1))\n",
    "    num_correct = tf.reduce_sum(in_top1)\n",
    "    return (batch_size - num_correct) / float(batch_size)\n",
    "  \n",
    "  def train_operation(self, global_step, total_loss, top1_error):\n",
    "    '''\n",
    "    Defines train operations\n",
    "    :param global_step: tensor variable with shape [1]\n",
    "    :param total_loss: tensor with shape [1]\n",
    "    :param top1_error: tensor with shape [1]\n",
    "    :return: two operations. Running train_op will do optimization once. Running train_ema_op\n",
    "    will generate the moving average of train error and train loss for tensorboard\n",
    "    '''\n",
    "    # Add train_loss, current learning rate and train error into the tensorboard summary ops\n",
    "    tf.summary.scalar('learning_rate', self.lr_placeholder)\n",
    "    tf.summary.scalar('train_loss', total_loss)\n",
    "    tf.summary.scalar('train_top1_error', top1_error)\n",
    "\n",
    "    # The ema object help calculate the moving average of train loss and train error\n",
    "    ema = tf.train.ExponentialMovingAverage(self.TRAIN_EMA_DECAY, global_step)\n",
    "    train_ema_op = ema.apply([total_loss, top1_error])\n",
    "    tf.summary.scalar('train_top1_error_avg', ema.average(top1_error))\n",
    "    tf.summary.scalar('train_loss_avg', ema.average(total_loss))\n",
    "\n",
    "    opt = tf.train.MomentumOptimizer(learning_rate=self.lr_placeholder, momentum=0.9)\n",
    "    train_op = opt.minimize(total_loss, global_step=global_step)\n",
    "    return train_op, train_ema_op\n",
    "\n",
    "  def validation_op(self, validation_step, top1_error, loss):\n",
    "    '''\n",
    "    Defines validation operations\n",
    "    :param validation_step: tensor with shape [1]\n",
    "    :param top1_error: tensor with shape [1]\n",
    "    :param loss: tensor with shape [1]\n",
    "    :return: validation operation\n",
    "    '''\n",
    "\n",
    "    # This ema object help calculate the moving average of validation loss and error\n",
    "\n",
    "    # ema with decay = 0.0 won't average things at all. This returns the original error\n",
    "    ema = tf.train.ExponentialMovingAverage(0.0, validation_step)\n",
    "    ema2 = tf.train.ExponentialMovingAverage(0.95, validation_step)\n",
    "\n",
    "\n",
    "    val_op = tf.group(validation_step.assign_add(1), ema.apply([top1_error, loss]), ema2.apply([top1_error, loss]))\n",
    "    top1_error_val = ema.average(top1_error)\n",
    "    top1_error_avg = ema2.average(top1_error)\n",
    "    loss_val = ema.average(loss)\n",
    "    loss_val_avg = ema2.average(loss)\n",
    "\n",
    "    # Summarize these values on tensorboard\n",
    "    tf.summary.scalar('val_top1_error', top1_error_val)\n",
    "    tf.summary.scalar('val_top1_error_avg', top1_error_avg)\n",
    "    tf.summary.scalar('val_loss', loss_val)\n",
    "    tf.summary.scalar('val_loss_avg', loss_val_avg)\n",
    "    \n",
    "    return val_op\n",
    "  \n",
    "  def generate_vali_batch(self, vali_data, vali_label, vali_batch_size):\n",
    "    '''\n",
    "    If you want to use a random batch of validation data to validate instead of using the\n",
    "    whole validation data, this function helps you generate that batch\n",
    "    :param vali_data: 4D numpy array\n",
    "    :param vali_label: 1D numpy array\n",
    "    :param vali_batch_size: int\n",
    "    :return: 4D numpy array and 1D numpy array\n",
    "    '''\n",
    "    offset = np.random.choice(100 - vali_batch_size, 1)[0]\n",
    "    vali_data_batch = vali_data[offset:offset+vali_batch_size, ...]\n",
    "    vali_label_batch = vali_label[offset:offset+vali_batch_size]\n",
    "    return vali_data_batch, vali_label_batch\n",
    "\n",
    "\n",
    "  def generate_augment_train_batch(self, train_data, train_labels, train_batch_size):\n",
    "    '''\n",
    "    This function helps generate a batch of train data\n",
    "    :param train_data: 4D numpy array\n",
    "    :param train_labels: 1D numpy array\n",
    "    :param train_batch_size: int\n",
    "    :return: augmented train batch data and labels. 4D numpy array and 1D numpy array\n",
    "    '''\n",
    "    \n",
    "    offset = np.random.choice(self.EPOCH_SIZE - train_batch_size, 1)[0]\n",
    "    batch_data = train_data[offset:offset + train_batch_size, ...]\n",
    "    batch_label = train_labels[offset:offset + self.BATCH_SIZE_TRAIN]\n",
    "    \n",
    "    return batch_data, batch_label\n",
    "    \n",
    "  def full_validation(self, loss, top1_error, session, vali_data, vali_labels, batch_data, batch_label):\n",
    "    '''\n",
    "    Runs validation on all the 10000 valdiation images\n",
    "    :param loss: tensor with shape [1]\n",
    "    :param top1_error: tensor with shape [1]\n",
    "    :param session: the current tensorflow session\n",
    "    :param vali_data: 4D numpy array\n",
    "    :param vali_labels: 1D numpy array\n",
    "    :param batch_data: 4D numpy array. training batch to feed dict and fetch the weights\n",
    "    :param batch_label: 1D numpy array. training labels to feed the dict\n",
    "    :return: float, float\n",
    "    '''\n",
    "    num_batches = 10000 // self.BATCH_SIZE_VAL\n",
    "    order = np.random.choice(10000, num_batches * self.BATCH_SIZE_VAL)\n",
    "    vali_data_subset = vali_data[order, ...]\n",
    "    vali_labels_subset = vali_labels[order]\n",
    "\n",
    "    loss_list = []\n",
    "    error_list = []\n",
    "\n",
    "    for step in range(num_batches):\n",
    "      offset = step * self.BATCH_SIZE_VAL\n",
    "      feed_dict = {self.traj_placeholder: batch_data, self.goal_placeholder: batch_label,\n",
    "        self.vali_traj_placeholder: vali_data_subset[offset:offset+self.BATCH_SIZE_VAL, ...],\n",
    "        self.vali_goal_placeholder: vali_labels_subset[offset:offset+self.BATCH_SIZE_VAL],\n",
    "        self.lr_placeholder: self.INIT_LR}\n",
    "      loss_value, top1_error_value = session.run([loss, top1_error], feed_dict=feed_dict)\n",
    "      loss_list.append(loss_value)\n",
    "      error_list.append(top1_error_value)\n",
    "\n",
    "    return np.mean(loss_list), np.mean(error_list)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = Model()\n",
    "    model.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
