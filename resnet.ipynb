{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "BN_EPSILON = 0.001\n",
    "WEIGHT_DECAY = 0.0002\n",
    "\n",
    "def activation_summary(x):\n",
    "    tensor_name = x.op.name\n",
    "    tf.summary.histogram(tensor_name + '/activations', x)\n",
    "    tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))\n",
    "\n",
    "def create_variables(name, shape, is_fc_layer, initializer=tf.contrib.layers.xavier_initializer()):\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(scale=WEIGHT_DECAY)\n",
    "    new_variables = tf.get_variable(name, shape=shape, initializer=initializer, regularizer=regularizer)\n",
    "    return new_variables\n",
    "\n",
    "def batch_normalization_layer(input_layer, dimension):\n",
    "    mean, variance = tf.nn.moments(input_layer, axes=[0, 1, 2])\n",
    "    beta = tf.get_variable('beta', dimension, tf.float32, initializer=tf.constant_initializer(0.0, tf.float32))\n",
    "    gamma = tf.get_variable('gamma', dimension, tf.float32, initializer=tf.constant_initializer(1.0, tf.float32))\n",
    "    bn_layer = tf.nn.batch_normalization(input_layer, mean, variance, beta, gamma, BN_EPSILON)\n",
    "\n",
    "    return bn_layer\n",
    "\n",
    "def conv_bn_relu_layer(input_layer, filter_shape, stride):\n",
    "    out_channel = filter_shape[-1]\n",
    "    filter = create_variables(name='conv', shape=filter_shape, is_fc_layer=False)\n",
    "\n",
    "    conv_layer = tf.nn.conv2d(input_layer, filter, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    bn_layer = batch_normalization_layer(conv_layer, out_channel)\n",
    "\n",
    "    output = tf.nn.relu(bn_layer)\n",
    "    return output\n",
    "\n",
    "def residual_block(input_layer, output_channels):\n",
    "    input_channel = input_layer.get_shape().as_list()[-1]\n",
    "    stride = 1\n",
    "\n",
    "    with tf.variable_scope('conv1_in_block'):\n",
    "        conv1 = conv_bn_relu_layer(input_layer, [3, 3, input_channel, output_channels], stride)\n",
    "\n",
    "    with tf.variable_scope('conv2_in_block'):\n",
    "        conv2 = conv_bn_relu_layer(conv1, [3, 3, output_channels, output_channels], stride)\n",
    "\n",
    "    output = conv2 + input_layer\n",
    "    \n",
    "    return output\n",
    "\n",
    "def average_pooling_layer(input_layer):\n",
    "    pooled_input = tf.nn.avg_pool(input_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    return pooled_input\n",
    "\n",
    "def lstm_layer(input_layer, mode):\n",
    "    num_hidden = 64\n",
    "    num_classes = 38\n",
    "    batch_size = 32\n",
    "    out_channels = 45\n",
    "    output_keep_prob = 0.8\n",
    "\n",
    "    _, feature_h, feature_w, _ = input_layer.get_shape().as_list()\n",
    "    print('\\nfeature_h: {}, feature_w: {}'.format(feature_h, feature_w))\n",
    "    \n",
    "    lstm_input = tf.transpose(input_layer,[0,2,1,3])\n",
    "    lstm_input = tf.reshape(lstm_input, [batch_size, feature_w, feature_h * out_channels])\n",
    "    seq_len = tf.fill([lstm_input.get_shape().as_list()[0]], feature_w)\n",
    "\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(num_hidden, state_is_tuple=True)\n",
    "    if mode == 'train':\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=output_keep_prob)\n",
    "\n",
    "    #cell1 = tf.nn.rnn_cell.LSTMCell(num_hidden, state_is_tuple=True)\n",
    "    #if mode == 'train':\n",
    "    #    cell1 = tf.nn.rnn_cell.DropoutWrapper(cell=cell1, output_keep_prob=output_keep_prob)\n",
    "    \n",
    "    #stack = tf.nn.rnn_cell.MultiRNNCell([cell, cell1], state_is_tuple=True)\n",
    "\n",
    "    initial_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "    outputs, _ = tf.nn.dynamic_rnn(cell=cell, inputs=lstm_input, sequence_length=seq_len, initial_state=initial_state, dtype=tf.float32, time_major=False)\n",
    "    outputs = tf.reshape(outputs, [-1, num_hidden])\n",
    "    \n",
    "    W = tf.get_variable(name='W_out', shape=[num_hidden, num_classes], dtype=tf.float32, initializer=tf.glorot_uniform_initializer())\n",
    "    b = tf.get_variable(name='b_out', shape=[num_classes], dtype=tf.float32, initializer=tf.constant_initializer())\n",
    "\n",
    "    #Linear output\n",
    "    lstm_h = tf.matmul(outputs, W) + b\n",
    "    shape = lstm_input.shape\n",
    "    lstm_h = tf.reshape(lstm_h, [shape[0], -1, num_classes])\n",
    "    return lstm_h\n",
    "\n",
    "def output_layer(input_layer, num_labels):\n",
    "    '''\n",
    "    :param input_layer: 2D tensor\n",
    "    :param num_labels: int. How many output labels in total?\n",
    "    :return: output layer Y = WX + B\n",
    "    '''\n",
    "    \n",
    "    input_dim = input_layer.get_shape().as_list()[-1]\n",
    "\n",
    "    fc_w = create_variables(name='fc_weights', shape=[input_dim, num_labels], is_fc_layer=True, initializer=tf.uniform_unit_scaling_initializer(factor=1.0))\n",
    "    fc_b = create_variables(name='fc_bias', shape=[num_labels], is_fc_layer=True, initializer=tf.zeros_initializer())\n",
    "    fc_h = tf.matmul(input_layer, fc_w) + fc_b\n",
    "\n",
    "    return fc_h\n",
    "\n",
    "def build_charnet(input_tensor, n, reuse, train):\n",
    "    layers = []\n",
    "       \n",
    "    #Append the input tensor as first layer\n",
    "    layers.append(input_tensor)\n",
    "    \n",
    "    #Add n residual layers\n",
    "    for i in range(n):\n",
    "        with tf.variable_scope('conv_%d' %i, reuse=reuse):\n",
    "            block = residual_block(layers[-1], 45)\n",
    "            activation_summary(block)\n",
    "            layers.append(block)\n",
    "    \n",
    "    #Add average pooling\n",
    "    with tf.variable_scope('average_pooling', reuse=reuse):\n",
    "        avg_pool = average_pooling_layer(block)\n",
    "        layers.append(avg_pool)\n",
    "    \n",
    "    #Wrap each layer in time distributed for LSTM\n",
    "        #for i in range(len(layers)):\n",
    "        #    layers[i] = tf.keras.layers.TimeDistributed(layers[i], input_shape=(batch_size, time_steps, 14, 14, 45))\n",
    "\n",
    "    with tf.variable_scope('LSTM', reuse=reuse):\n",
    "        #Add LSTM layer\n",
    "        lstm = lstm_layer(layers[-1], train)\n",
    "        layers.append(lstm)\n",
    "        \n",
    "    #One convolutional layer\n",
    "\n",
    "\n",
    "    #Fully connected\n",
    "    with tf.variable_scope('fc', reuse=reuse):\n",
    "        in_channel = layers[-1].get_shape().as_list()[-1]\n",
    "        #bn_layer = batch_normalization_layer(layers[-1], in_channel)\n",
    "        #print('bn layer', bn_layer)\n",
    "        #relu_layer = tf.nn.relu(bn_layer)\n",
    "        #print('relu_layer', relu_layer)\n",
    "        global_pool = tf.reduce_mean(layers[-1], [1])\n",
    "        assert global_pool.get_shape().as_list()[-1:] == [38] #Originally 64, 45 without LSTM\n",
    "        output = output_layer(global_pool, 38)\n",
    "        layers.append(output)\n",
    "    \n",
    "    return layers[-1]\n",
    "\n",
    "def build_pred_head(self):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
