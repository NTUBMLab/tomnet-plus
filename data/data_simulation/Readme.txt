--------------------------
Note
--------------------------
generated by the script at 'tomnet-project\scripts\simulation_data_generator\simulation_data_generator.py'

--------------------------
Version
--------------------------
S002:
- 38 potential targets in the maze.
S002a:
- 1 ~ 4 potential targets in the maze.
- This is the version that all results in the 'working_model' is trained on, because 
38 targets are just way too hard for the model to learn. Plus, 4 goals are in consistence with the ToMNET paper.
- Based on 'S002a_familyonly.csv'.

S002b:
- Like S002a but all mazed have exact 4 goals.
- This is to make the task easier for model to learn. It is impossible for a
model to infer the target by a qeury state if the trajectory the model saw
contained only 1 goal.
- Based on 'S002b_familyonly.csv'.

S003b:
- Like S002b except that it's based on 'tomnet-project\simulation_data_generator\S003b.csv'


simulation_data_on_server

--------------------------------
simulation_data_on_server
--------------------------------
--------------------------------
simulation_data_on_server/data/data_simulation/S004-S033/
--------------------------------
- Elaine generated this on 2020/03/04
- generated by 'scripts\simulation_data_generator\simulation_data_generator_social_reward.py' 
based on the csvs in the dir 'simulation_data_on_server/36agents/',
which are generated by 'scripts\simulation_data_generator\simulation_social_reward_generator.r'.
- the goal is to test the robustness of the preference inference under different distribution of social rewards.
- the agent acts according to (social reward) - (physical distance), or, u-d.
- commit 130306 (for the csvs)
- the dir is git ignored because it is too large.
--------------------------------
simulation_data_on_server/data/data_simulation/fromhuman/
--------------------------------
- Elaine generated this on 2020/03/04
- generated by 'scripts\simulation_data_generator\simulation_data_generator_social_reward.py' 
based on the csvs in the dir 'simulation_data_on_server/agents_from_human/',
which are the scores that each human player see in the game.
- the goal is to test model performance given the human players' distribution of u.
- the agent acts according to (social reward) - (physical distance), or, u-d.
- commit 130306 (for the csvs)
- the dir is git ignored because it is too large.

--------------------------------
S004b-S037b (not used because it uses the rule u(log(u))-d(log(d) instead of (u-d))
--------------------------------
- generated by 'scripts\simulation_data_generator\simulation_data_generator_social_reward.py' 
based on the csvs in the dir 'scripts\simulation_data_generator\34agents',
which are generated by 'scripts\simulation_data_generator\simulation_social_reward_generator.r'.
- commit 2ceaa8 (for the csvs)
- the dir is git ignored because it is too large.
 
